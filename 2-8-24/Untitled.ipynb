{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aaf5544-2ef7-4740-93cf-4b4e5bf06aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e989c574-03eb-448a-ad1e-75f2e161e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gTTS(\"My name is Manisha\")\n",
    "a.save(\"text.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a92f5e-6923-4b2b-b43c-92086d9755eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new.txt','r')as file: \n",
    "    b = file.read()\n",
    "\n",
    "c = gTTS(b, lang='en')\n",
    "c.save(\"txt1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdbe417-93bb-4c52-8cb7-af71da93c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gTTS('हैलो दोस्तों। मैं मनीषा हूं. मैं पानीपत हरियाणा से हूं.')\n",
    "d.save(\"txt2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc8023-5fde-4d1c-93da-dcafffa93c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentation & tokenizen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586f67e6-876d-472c-9972-345f54495167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f5572f-619b-4405-9e7e-9e654298ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad81192-91dd-4562-956f-9e9bd2dbb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e887ea-18ea-4072-bc9c-f19269091eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ('We define short form poetry. as anything 9 lines and under, or any poem. that uses 60 words or less. The sonnet, for example, is a 14-line poem that often grapples with love, and though sonnets are by no means “long,” they often have abstract qualities not found in short poems.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfa3ac0-3e63-4fd7-9702-b83d5a7d9fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We define short form poetry. as anything 9 lines and under, or any poem. that uses 60 words or less. The sonnet, for example, is a 14-line poem that often grapples with love, and though sonnets are by no means “long,” they often have abstract qualities not found in short poems.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f97f3f-66f0-4ec6-95b9-3299aab1126f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We define short form poetry.',\n",
       " 'as anything 9 lines and under, or any poem.',\n",
       " 'that uses 60 words or less.',\n",
       " 'The sonnet, for example, is a 14-line poem that often grapples with love, and though sonnets are by no means “long,” they often have abstract qualities not found in short poems.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec788129-eeda-402a-9955-b8c192fb6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "b = nlp(a)\n",
    "word_tokens = [token.text for token in b]\n",
    "sent_tokens = [sent.text for sent in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99177e20-7f76-413a-a36a-e8d6e9dd6090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'define',\n",
       " 'short',\n",
       " 'form',\n",
       " 'poetry',\n",
       " '.',\n",
       " 'as',\n",
       " 'anything',\n",
       " '9',\n",
       " 'lines',\n",
       " 'and',\n",
       " 'under',\n",
       " ',',\n",
       " 'or',\n",
       " 'any',\n",
       " 'poem',\n",
       " '.',\n",
       " 'that',\n",
       " 'uses',\n",
       " '60',\n",
       " 'words',\n",
       " 'or',\n",
       " 'less',\n",
       " '.',\n",
       " 'The',\n",
       " 'sonnet',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " '14',\n",
       " '-',\n",
       " 'line',\n",
       " 'poem',\n",
       " 'that',\n",
       " 'often',\n",
       " 'grapples',\n",
       " 'with',\n",
       " 'love',\n",
       " ',',\n",
       " 'and',\n",
       " 'though',\n",
       " 'sonnets',\n",
       " 'are',\n",
       " 'by',\n",
       " 'no',\n",
       " 'means',\n",
       " '“',\n",
       " 'long',\n",
       " ',',\n",
       " '”',\n",
       " 'they',\n",
       " 'often',\n",
       " 'have',\n",
       " 'abstract',\n",
       " 'qualities',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'short',\n",
       " 'poems',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601eb62e-6d90-4022-92ff-3cd8b4814ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'define',\n",
       " 'short',\n",
       " 'form',\n",
       " 'poetry',\n",
       " '.',\n",
       " 'as',\n",
       " 'anything',\n",
       " '9',\n",
       " 'lines',\n",
       " 'and',\n",
       " 'under',\n",
       " ',',\n",
       " 'or',\n",
       " 'any',\n",
       " 'poem',\n",
       " '.',\n",
       " 'that',\n",
       " 'uses',\n",
       " '60',\n",
       " 'words',\n",
       " 'or',\n",
       " 'less',\n",
       " '.',\n",
       " 'The',\n",
       " 'sonnet',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " '14',\n",
       " '-',\n",
       " 'line',\n",
       " 'poem',\n",
       " 'that',\n",
       " 'often',\n",
       " 'grapples',\n",
       " 'with',\n",
       " 'love',\n",
       " ',',\n",
       " 'and',\n",
       " 'though',\n",
       " 'sonnets',\n",
       " 'are',\n",
       " 'by',\n",
       " 'no',\n",
       " 'means',\n",
       " '“',\n",
       " 'long',\n",
       " ',',\n",
       " '”',\n",
       " 'they',\n",
       " 'often',\n",
       " 'have',\n",
       " 'abstract',\n",
       " 'qualities',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'short',\n",
       " 'poems',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f5b1c39-bb52-4be1-907b-16b59fb6e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc73369d-b887-4421-a4af-933a3165b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67f4e384-d1f9-4128-b7bb-442be0e2dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eats\n",
      "eating\n",
      "eatery\n",
      "is\n",
      "happy\n",
      "happily\n",
      "happiness\n"
     ]
    }
   ],
   "source": [
    "words = ['eat','eats','eating','eatery','is','happy','happily','happiness']\n",
    "for a in words: \n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1369b51-1fc5-4f85-89ee-9aee58e53311",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d251e82-c69d-46ed-9740-ee65d7e403ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eat', 'eat', 'eateri', 'is', 'happi', 'happili', 'happi']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stems = [p.stem(w) for w in words]\n",
    "p_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0612051e-7932-4fa8-921c-9b13b97d25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SnowballStemmer(language= 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8efd7d4-8c0b-44d0-b5fb-78bce31708ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eat', 'eat', 'eateri', 'is', 'happi', 'happili', 'happi']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d4844c6-a05c-4862-902c-ae82bf6d0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8f73a06-27c1-4f39-869a-e8c3a4cd0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem  import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2959c50e-02a9-4ef3-be74-6b56c85dfc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eats', 'eating', 'eatery', 'is', 'happy', 'happily', 'happiness']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98a20382-40ba-437c-8bd3-79fdb6842e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a13e6fd4-68a8-45a1-b7dc-1d7165ba6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "574f304a-1d8d-40fa-b800-2d2034b32d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bf4d94b-9c00-4d2c-b287-595eb6228915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f62eb47d-747f-4d46-9fc7-3159b3820f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = [l.lemmatize(w, pos = wordnet.NOUN) for w in words] # firnd which one is noun or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7413f16-7f02-4043-b2e2-b0df960add67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eats', 'eating', 'eatery', 'is', 'happy', 'happily', 'happiness']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbe73a8e-6077-4e60-af1d-adde56dd2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8df0faa-e542-4773-ac2b-4c14330cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eat','eating','eats','ate','happy','meeting','rafting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbdd4e23-723b-4f32-b31a-f0371ec9f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS -- ROOTS\n",
      "eat  --  eat\n",
      "eating  --  eat\n",
      "eats  --  eat\n",
      "ate  --  ate\n",
      "happy  --  happi\n",
      "meeting  --  meet\n",
      "rafting  --  raft\n"
     ]
    }
   ],
   "source": [
    "print('WORDS -- ROOTS')\n",
    "for i in words: \n",
    "    print(i, \" -- \", s.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90c6f8e3-e85a-4fff-82c4-aae92ecc0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d629896-2112-442a-9702-c630ac77ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b900165d-9ddf-4842-acd3-a789fae6d0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eating', 'eats', 'ate', 'happy', 'meeting', 'rafting']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.lemmatize(w, pos=wordnet.NOUN)  for w in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1769a292-0ad7-4482-9282-c7efe3bfec1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'eat', 'eat', 'eat', 'happy', 'meet', 'raft']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.lemmatize(w, pos=wordnet.VERB)for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13fe4dec-68cd-4ce2-89d8-477321d0ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28c1199a-1c01-4f28-a11d-d950d9020d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f23c7b3-e9d9-48ca-bbae-fadf943eef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This structure ensures a clear and concise presentation, highlighting the core aspects of both Critical Thinking and Design Thinking. Would you like me to provide more detailed content for each slide?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59bfb37d-f402-44a8-97cc-3e459e1886ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('provide', 'VB'),\n",
       " ('detailed', 'JJ'),\n",
       " ('content', 'NN'),\n",
       " ('slide', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = sent_tokenize(text)\n",
    "for i in token: \n",
    "    WL = nltk.word_tokenize(i)\n",
    "    WL = [w for w in WL if not w in stop_w]\n",
    "    tagg = nltk.pos_tag(WL)\n",
    "tagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d4d1d025-b2d1-4511-af52-82589ffe9773",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c080b134-7979-478a-ad6a-73709ae62ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fc699d17-d1b8-4cdf-b050-6cb720593cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am Manisha.\", \"I am from Panipat.\",\" I am in ADIT course. \"]\n",
    "x = v.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12111e0a-7797-4b96-a45c-209cf4462708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 1,\n",
       " 'manisha': 5,\n",
       " 'from': 3,\n",
       " 'panipat': 6,\n",
       " 'in': 4,\n",
       " 'adit': 0,\n",
       " 'course': 2}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff2830f3-fe02-47aa-ac8d-71830f163a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f3afaa03-b23c-4ff4-baef-c43da1b286eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.50854232, 0.        , 0.        , 0.        ,\n",
       "        0.861037  , 0.        ],\n",
       "       [0.        , 0.38537163, 0.        , 0.65249088, 0.        ,\n",
       "        0.        , 0.65249088],\n",
       "       [0.54645401, 0.32274454, 0.54645401, 0.        , 0.54645401,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a37403d9-c1d0-4eb6-a224-e670cb25e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "10ffb244-01e8-4b2e-9987-40f795f1a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [word_tokenize(w.lower()) for w in text]\n",
    "s\n",
    "model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d7336250-35d4-47a8-a113-1a86424a5d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word3Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWord3Vec\u001b[49m(sentences \u001b[38;5;241m=\u001b[39m s, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, sg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word3Vec' is not defined"
     ]
    }
   ],
   "source": [
    "model = Word3Vec(sentences = s, vector_size=5, workers = 5, sg = 0)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a9894-df0d-4ad2-b986-aa6117416773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa78ca3-d004-4da3-897c-0f776eaa29a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
